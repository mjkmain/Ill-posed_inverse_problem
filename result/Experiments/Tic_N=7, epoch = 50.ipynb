{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "36076719",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "#tf.debugging.set_log_device_placement(True)\n",
    "#gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "#print(len(gpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "64f67c81",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "1d5d5474",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>-0.00850392</th>\n",
       "      <th>-0.05519372</th>\n",
       "      <th>-0.08868060</th>\n",
       "      <th>0.20194264</th>\n",
       "      <th>0.12567687</th>\n",
       "      <th>-0.07163637</th>\n",
       "      <th>0.17574831</th>\n",
       "      <th>-0.00712658</th>\n",
       "      <th>0.00692815</th>\n",
       "      <th>-0.03070841</th>\n",
       "      <th>...</th>\n",
       "      <th>-0.13620685</th>\n",
       "      <th>0.03215167</th>\n",
       "      <th>0.06394811</th>\n",
       "      <th>0.12541900</th>\n",
       "      <th>-0.15735441</th>\n",
       "      <th>-0.04155950</th>\n",
       "      <th>-0.07144600</th>\n",
       "      <th>-0.08675600</th>\n",
       "      <th>0.02004123</th>\n",
       "      <th>0.05684330</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.339310e-03</td>\n",
       "      <td>4.925380e-03</td>\n",
       "      <td>-0.006329</td>\n",
       "      <td>-3.951720e-03</td>\n",
       "      <td>-6.602700e-03</td>\n",
       "      <td>-5.497060e-03</td>\n",
       "      <td>4.022900e-03</td>\n",
       "      <td>-0.009091</td>\n",
       "      <td>5.002900e-04</td>\n",
       "      <td>0.002524</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.809640e-03</td>\n",
       "      <td>-5.475300e-04</td>\n",
       "      <td>6.055160e-03</td>\n",
       "      <td>-9.573900e-04</td>\n",
       "      <td>-3.682100e-04</td>\n",
       "      <td>-4.390490e-03</td>\n",
       "      <td>3.805900e-04</td>\n",
       "      <td>8.492540e-03</td>\n",
       "      <td>-4.146920e-03</td>\n",
       "      <td>-4.295300e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-2.519000e-05</td>\n",
       "      <td>-5.390000e-06</td>\n",
       "      <td>0.000033</td>\n",
       "      <td>3.833000e-05</td>\n",
       "      <td>-3.995000e-05</td>\n",
       "      <td>9.930000e-06</td>\n",
       "      <td>8.360000e-06</td>\n",
       "      <td>-0.000003</td>\n",
       "      <td>-1.596000e-05</td>\n",
       "      <td>-0.000015</td>\n",
       "      <td>...</td>\n",
       "      <td>2.748000e-05</td>\n",
       "      <td>-1.066000e-05</td>\n",
       "      <td>1.960000e-06</td>\n",
       "      <td>-6.340000e-06</td>\n",
       "      <td>1.088000e-05</td>\n",
       "      <td>2.879000e-05</td>\n",
       "      <td>-2.530000e-06</td>\n",
       "      <td>-3.686000e-05</td>\n",
       "      <td>2.208000e-05</td>\n",
       "      <td>1.735000e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.000000e-08</td>\n",
       "      <td>-1.000000e-08</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000e-08</td>\n",
       "      <td>-1.000000e-08</td>\n",
       "      <td>2.000000e-08</td>\n",
       "      <td>-2.000000e-08</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000e-08</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.000000e-08</td>\n",
       "      <td>-4.000000e-08</td>\n",
       "      <td>5.000000e-08</td>\n",
       "      <td>-5.000000e-08</td>\n",
       "      <td>-3.000000e-08</td>\n",
       "      <td>3.000000e-08</td>\n",
       "      <td>-1.000000e-08</td>\n",
       "      <td>-4.000000e-08</td>\n",
       "      <td>5.000000e-08</td>\n",
       "      <td>1.000000e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000e+00</td>\n",
       "      <td>-0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>-0.000000e+00</td>\n",
       "      <td>-0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>-0.000000e+00</td>\n",
       "      <td>-0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>-0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>-0.000000e+00</td>\n",
       "      <td>-0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>-0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>-0.000000e+00</td>\n",
       "      <td>-0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>-0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>-0.000000e+00</td>\n",
       "      <td>-0.000000e+00</td>\n",
       "      <td>-0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>-0.000000e+00</td>\n",
       "      <td>-0.000000e+00</td>\n",
       "      <td>-0.000000e+00</td>\n",
       "      <td>-0.000000e+00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6 rows × 100000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    -0.00850392   -0.05519372  -0.08868060    0.20194264    0.12567687  \\\n",
       "0  4.339310e-03  4.925380e-03    -0.006329 -3.951720e-03 -6.602700e-03   \n",
       "1 -2.519000e-05 -5.390000e-06     0.000033  3.833000e-05 -3.995000e-05   \n",
       "2  2.000000e-08 -1.000000e-08     0.000000 -2.000000e-08 -1.000000e-08   \n",
       "3 -0.000000e+00  0.000000e+00     0.000000 -0.000000e+00 -0.000000e+00   \n",
       "4  0.000000e+00  0.000000e+00     0.000000  0.000000e+00 -0.000000e+00   \n",
       "5 -0.000000e+00  0.000000e+00     0.000000 -0.000000e+00  0.000000e+00   \n",
       "\n",
       "    -0.07163637    0.17574831  -0.00712658    0.00692815  -0.03070841  ...  \\\n",
       "0 -5.497060e-03  4.022900e-03    -0.009091  5.002900e-04     0.002524  ...   \n",
       "1  9.930000e-06  8.360000e-06    -0.000003 -1.596000e-05    -0.000015  ...   \n",
       "2  2.000000e-08 -2.000000e-08     0.000000 -2.000000e-08     0.000000  ...   \n",
       "3  0.000000e+00  0.000000e+00    -0.000000  0.000000e+00     0.000000  ...   \n",
       "4 -0.000000e+00  0.000000e+00    -0.000000  0.000000e+00    -0.000000  ...   \n",
       "5  0.000000e+00 -0.000000e+00     0.000000  0.000000e+00     0.000000  ...   \n",
       "\n",
       "    -0.13620685    0.03215167    0.06394811    0.12541900   -0.15735441  \\\n",
       "0 -1.809640e-03 -5.475300e-04  6.055160e-03 -9.573900e-04 -3.682100e-04   \n",
       "1  2.748000e-05 -1.066000e-05  1.960000e-06 -6.340000e-06  1.088000e-05   \n",
       "2 -1.000000e-08 -4.000000e-08  5.000000e-08 -5.000000e-08 -3.000000e-08   \n",
       "3  0.000000e+00 -0.000000e+00 -0.000000e+00  0.000000e+00 -0.000000e+00   \n",
       "4 -0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "5  0.000000e+00 -0.000000e+00 -0.000000e+00 -0.000000e+00  0.000000e+00   \n",
       "\n",
       "    -0.04155950   -0.07144600   -0.08675600    0.02004123    0.05684330  \n",
       "0 -4.390490e-03  3.805900e-04  8.492540e-03 -4.146920e-03 -4.295300e-03  \n",
       "1  2.879000e-05 -2.530000e-06 -3.686000e-05  2.208000e-05  1.735000e-05  \n",
       "2  3.000000e-08 -1.000000e-08 -4.000000e-08  5.000000e-08  1.000000e-08  \n",
       "3 -0.000000e+00  0.000000e+00 -0.000000e+00  0.000000e+00  0.000000e+00  \n",
       "4  0.000000e+00 -0.000000e+00  0.000000e+00 -0.000000e+00 -0.000000e+00  \n",
       "5  0.000000e+00 -0.000000e+00 -0.000000e+00 -0.000000e+00 -0.000000e+00  \n",
       "\n",
       "[6 rows x 100000 columns]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#data load, F.txt 파일이 있는 절대 경로를 입력하시면 됩니다.\n",
    "#data 확인을 위하여 출력해봅니다. column name에 데이터가 들어가있어서 처리해주어야 합니다.\n",
    "# N = 5\n",
    "data = pd.read_csv('C:/Users/Administrator/Documents/MATLAB/Data set/F.txt', sep = '\\t')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "aaecaead",
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = []\n",
    "for i in range(len(data.iloc[0,:])):\n",
    "    arr.append(i+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "130e80da",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load data\n",
    "#column name에 data가 들어가있기 때문에 \"names = arr\" 로 처리해 줍니다. arr은 1 ~ len(F)의 숫자가 담겨있습니다.\n",
    "dataF = pd.read_csv('C:/Users/Administrator/Documents/MATLAB/Data set/F.txt', sep = '\\t', names = arr)\n",
    "dataQ = pd.read_csv('C:/Users/Administrator/Documents/MATLAB/Data set/Q.txt', sep = '\\t', names = arr)\n",
    "data_nF = pd.read_csv('C:/Users/Administrator/Documents/MATLAB/Data set/nF.txt', sep = '\\t', names = arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "0e41b12a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_arr(A):\n",
    "    arA = []\n",
    "    for j in range(len(A.iloc[0, :])):\n",
    "        arA1 = []\n",
    "        for i in range(len(A.iloc[:,0])):\n",
    "            tmpA = A.iloc[:,j][i]\n",
    "            arA1.append(tmpA)\n",
    "        arA.append(arA1)\n",
    "    return arA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "06b62f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#make array\n",
    "arrF = make_arr(dataF)\n",
    "dataF = np.array(arrF)\n",
    "\n",
    "arrQ = make_arr(dataQ)\n",
    "dataQ = np.array(arrQ)\n",
    "\n",
    "arr_nF = make_arr(data_nF)\n",
    "data_nF = np.array(arr_nF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "bebffb63",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# N = 5\n",
    "A = []\n",
    "for n in range(1,6):\n",
    "    x = np.exp(-(n**2))\n",
    "    A.append(x)\n",
    "A = np.diag(A)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "01383513",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 7), (20000, 7), (20000, 7)\n",
      "(60000, 7), (20000, 7), (20000, 7)\n",
      "(60000, 7), (20000, 7), (20000, 7)\n"
     ]
    }
   ],
   "source": [
    "# data 분할\n",
    "\n",
    "import math\n",
    "train_size = math.floor(len(dataF)*0.6) # train : 60%\n",
    "val_size = math.floor(len(dataF)*0.2) #val : 20%\n",
    "test_size = math.floor(len(dataF)*0.2) #test : 20%\n",
    "#generate F_data, F_val, F_test\n",
    "F_data = dataF[:train_size, :]\n",
    "F_val = dataF[train_size:(val_size + train_size), :]\n",
    "F_test = dataF[(val_size + train_size):(val_size + train_size + test_size), :]\n",
    "\n",
    "#generate Q_data, Q_val, Q_test\n",
    "Q_data = dataQ[:train_size, :]\n",
    "Q_val = dataQ[train_size:(val_size + train_size), :]\n",
    "Q_test = dataQ[(val_size + train_size):(val_size + train_size + test_size), :]\n",
    "\n",
    "#generate nF_data, nF_val, nF_test\n",
    "nF_data = data_nF[:train_size, :]\n",
    "nF_val = data_nF[train_size:(val_size + train_size), :]\n",
    "nF_test = data_nF[(val_size + train_size):(val_size + train_size + test_size), :]\n",
    "\n",
    "print(f'{F_data.shape}, {F_test.shape}, {F_val.shape}')\n",
    "print(f'{Q_data.shape}, {Q_test.shape}, {Q_val.shape}')\n",
    "print(f'{nF_data.shape}, {nF_test.shape}, {nF_val.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "d4e02b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "model = keras.models.Sequential() #Sequentioal\n",
    "\n",
    "model.add(keras.layers.Dense(input_dim = 7, units = 10, activation= \"tanh\",kernel_initializer =\"HeNormal\" )) \n",
    "model.add(keras.layers.Dense(20, activation= \"tanh\")) \n",
    "model.add(keras.layers.Dense(50, activation= \"tanh\"))  \n",
    "model.add(keras.layers.Dense(100, activation= \"tanh\")) \n",
    "model.add(keras.layers.Dense(120, activation= \"tanh\")) \n",
    "model.add(keras.layers.Dense(50, activation= \"tanh\"))\n",
    "model.add(keras.layers.Dense(7, activation= \"tanh\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "b02072db",
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device('/GPU:0'):\n",
    "    F_train = tf.constant(F_data)\n",
    "    Q_train = tf.constant(Q_data)\n",
    "    nF_train = tf.constant(nF_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "3ea283d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "1875/1875 [==============================] - 1s 508us/step - loss: 0.0511 - accuracy: 0.3329 - val_loss: 0.0505 - val_accuracy: 0.3333\n",
      "Epoch 2/50\n",
      "1875/1875 [==============================] - 1s 485us/step - loss: 0.0507 - accuracy: 0.3330 - val_loss: 0.0507 - val_accuracy: 0.3232\n",
      "Epoch 3/50\n",
      "1875/1875 [==============================] - 1s 480us/step - loss: 0.0507 - accuracy: 0.3326 - val_loss: 0.0506 - val_accuracy: 0.3356\n",
      "Epoch 4/50\n",
      "1875/1875 [==============================] - 1s 479us/step - loss: 0.0507 - accuracy: 0.3322 - val_loss: 0.0506 - val_accuracy: 0.3363\n",
      "Epoch 5/50\n",
      "1875/1875 [==============================] - 1s 479us/step - loss: 0.0506 - accuracy: 0.3319 - val_loss: 0.0505 - val_accuracy: 0.3305\n",
      "Epoch 6/50\n",
      "1875/1875 [==============================] - 1s 484us/step - loss: 0.0506 - accuracy: 0.3327 - val_loss: 0.0505 - val_accuracy: 0.3276\n",
      "Epoch 7/50\n",
      "1875/1875 [==============================] - 1s 480us/step - loss: 0.0506 - accuracy: 0.3337 - val_loss: 0.0505 - val_accuracy: 0.3327\n",
      "Epoch 8/50\n",
      "1875/1875 [==============================] - 1s 478us/step - loss: 0.0506 - accuracy: 0.3336 - val_loss: 0.0506 - val_accuracy: 0.3302\n",
      "Epoch 9/50\n",
      "1875/1875 [==============================] - 1s 488us/step - loss: 0.0506 - accuracy: 0.3327 - val_loss: 0.0505 - val_accuracy: 0.3347\n",
      "Epoch 10/50\n",
      "1875/1875 [==============================] - 1s 488us/step - loss: 0.0506 - accuracy: 0.3321 - val_loss: 0.0505 - val_accuracy: 0.3291\n",
      "Epoch 11/50\n",
      "1875/1875 [==============================] - 1s 477us/step - loss: 0.0506 - accuracy: 0.3339 - val_loss: 0.0505 - val_accuracy: 0.3336\n",
      "Epoch 12/50\n",
      "1875/1875 [==============================] - 1s 479us/step - loss: 0.0506 - accuracy: 0.3343 - val_loss: 0.0505 - val_accuracy: 0.3343\n",
      "Epoch 13/50\n",
      "1875/1875 [==============================] - 1s 483us/step - loss: 0.0506 - accuracy: 0.3322 - val_loss: 0.0505 - val_accuracy: 0.3313\n",
      "Epoch 14/50\n",
      "1875/1875 [==============================] - 1s 481us/step - loss: 0.0506 - accuracy: 0.3338 - val_loss: 0.0505 - val_accuracy: 0.3324\n",
      "Epoch 15/50\n",
      "1875/1875 [==============================] - 1s 480us/step - loss: 0.0506 - accuracy: 0.3348 - val_loss: 0.0505 - val_accuracy: 0.3313\n",
      "Epoch 16/50\n",
      "1875/1875 [==============================] - 1s 484us/step - loss: 0.0506 - accuracy: 0.3340 - val_loss: 0.0505 - val_accuracy: 0.3331\n",
      "Epoch 17/50\n",
      "1875/1875 [==============================] - 1s 486us/step - loss: 0.0506 - accuracy: 0.3336 - val_loss: 0.0505 - val_accuracy: 0.3325\n",
      "Epoch 18/50\n",
      "1875/1875 [==============================] - 1s 477us/step - loss: 0.0506 - accuracy: 0.3335 - val_loss: 0.0505 - val_accuracy: 0.3288\n",
      "Epoch 19/50\n",
      "1875/1875 [==============================] - 1s 491us/step - loss: 0.0506 - accuracy: 0.3334 - val_loss: 0.0505 - val_accuracy: 0.3289\n",
      "Epoch 20/50\n",
      "1875/1875 [==============================] - 1s 495us/step - loss: 0.0506 - accuracy: 0.3336 - val_loss: 0.0505 - val_accuracy: 0.3331\n",
      "Epoch 21/50\n",
      "1875/1875 [==============================] - 1s 477us/step - loss: 0.0506 - accuracy: 0.3331 - val_loss: 0.0505 - val_accuracy: 0.3307\n",
      "Epoch 22/50\n",
      "1875/1875 [==============================] - 1s 477us/step - loss: 0.0506 - accuracy: 0.3344 - val_loss: 0.0505 - val_accuracy: 0.3306\n",
      "Epoch 23/50\n",
      "1875/1875 [==============================] - 1s 491us/step - loss: 0.0506 - accuracy: 0.3336 - val_loss: 0.0505 - val_accuracy: 0.3302\n",
      "Epoch 24/50\n",
      "1875/1875 [==============================] - 1s 476us/step - loss: 0.0506 - accuracy: 0.3343 - val_loss: 0.0505 - val_accuracy: 0.3331\n",
      "Epoch 25/50\n",
      "1875/1875 [==============================] - 1s 482us/step - loss: 0.0506 - accuracy: 0.3336 - val_loss: 0.0505 - val_accuracy: 0.3320\n",
      "Epoch 26/50\n",
      "1875/1875 [==============================] - 1s 479us/step - loss: 0.0506 - accuracy: 0.3329 - val_loss: 0.0505 - val_accuracy: 0.3293\n",
      "Epoch 27/50\n",
      "1875/1875 [==============================] - 1s 488us/step - loss: 0.0506 - accuracy: 0.3333 - val_loss: 0.0505 - val_accuracy: 0.3320\n",
      "Epoch 28/50\n",
      "1875/1875 [==============================] - 1s 483us/step - loss: 0.0506 - accuracy: 0.3350 - val_loss: 0.0505 - val_accuracy: 0.3327\n",
      "Epoch 29/50\n",
      "1875/1875 [==============================] - 1s 510us/step - loss: 0.0506 - accuracy: 0.3342 - val_loss: 0.0505 - val_accuracy: 0.3321\n",
      "Epoch 30/50\n",
      "1875/1875 [==============================] - 1s 480us/step - loss: 0.0506 - accuracy: 0.3350 - val_loss: 0.0505 - val_accuracy: 0.3331\n",
      "Epoch 31/50\n",
      "1875/1875 [==============================] - 1s 473us/step - loss: 0.0506 - accuracy: 0.3335 - val_loss: 0.0505 - val_accuracy: 0.3326\n",
      "Epoch 32/50\n",
      "1875/1875 [==============================] - 1s 479us/step - loss: 0.0506 - accuracy: 0.3341 - val_loss: 0.0505 - val_accuracy: 0.3332\n",
      "Epoch 33/50\n",
      "1875/1875 [==============================] - 1s 475us/step - loss: 0.0506 - accuracy: 0.3338 - val_loss: 0.0505 - val_accuracy: 0.3340\n",
      "Epoch 34/50\n",
      "1875/1875 [==============================] - 1s 472us/step - loss: 0.0506 - accuracy: 0.3344 - val_loss: 0.0505 - val_accuracy: 0.3323\n",
      "Epoch 35/50\n",
      "1875/1875 [==============================] - 1s 477us/step - loss: 0.0506 - accuracy: 0.3353 - val_loss: 0.0505 - val_accuracy: 0.3336\n",
      "Epoch 36/50\n",
      "1875/1875 [==============================] - 1s 470us/step - loss: 0.0464 - accuracy: 0.3872 - val_loss: 0.0406 - val_accuracy: 0.4441\n",
      "Epoch 37/50\n",
      "1875/1875 [==============================] - 1s 476us/step - loss: 0.0407 - accuracy: 0.4491 - val_loss: 0.0410 - val_accuracy: 0.4429\n",
      "Epoch 38/50\n",
      "1875/1875 [==============================] - 1s 487us/step - loss: 0.0407 - accuracy: 0.4513 - val_loss: 0.0405 - val_accuracy: 0.4453\n",
      "Epoch 39/50\n",
      "1875/1875 [==============================] - 1s 483us/step - loss: 0.0406 - accuracy: 0.4526 - val_loss: 0.0406 - val_accuracy: 0.4468\n",
      "Epoch 40/50\n",
      "1875/1875 [==============================] - 1s 485us/step - loss: 0.0406 - accuracy: 0.4513 - val_loss: 0.0406 - val_accuracy: 0.4469\n",
      "Epoch 41/50\n",
      "1875/1875 [==============================] - 1s 478us/step - loss: 0.0406 - accuracy: 0.4508 - val_loss: 0.0406 - val_accuracy: 0.4466\n",
      "Epoch 42/50\n",
      "1875/1875 [==============================] - 1s 476us/step - loss: 0.0406 - accuracy: 0.4518 - val_loss: 0.0405 - val_accuracy: 0.4515\n",
      "Epoch 43/50\n",
      "1875/1875 [==============================] - 1s 476us/step - loss: 0.0405 - accuracy: 0.4529 - val_loss: 0.0407 - val_accuracy: 0.4430\n",
      "Epoch 44/50\n",
      "1875/1875 [==============================] - 1s 491us/step - loss: 0.0405 - accuracy: 0.4533 - val_loss: 0.0405 - val_accuracy: 0.4493\n",
      "Epoch 45/50\n",
      "1875/1875 [==============================] - 1s 476us/step - loss: 0.0406 - accuracy: 0.4529 - val_loss: 0.0405 - val_accuracy: 0.4511\n",
      "Epoch 46/50\n",
      "1875/1875 [==============================] - 1s 472us/step - loss: 0.0405 - accuracy: 0.4521 - val_loss: 0.0405 - val_accuracy: 0.4527\n",
      "Epoch 47/50\n",
      "1875/1875 [==============================] - 1s 475us/step - loss: 0.0405 - accuracy: 0.4535 - val_loss: 0.0408 - val_accuracy: 0.4424\n",
      "Epoch 48/50\n",
      "1875/1875 [==============================] - 1s 474us/step - loss: 0.0405 - accuracy: 0.4534 - val_loss: 0.0405 - val_accuracy: 0.4516\n",
      "Epoch 49/50\n",
      "1875/1875 [==============================] - 1s 473us/step - loss: 0.0405 - accuracy: 0.4541 - val_loss: 0.0405 - val_accuracy: 0.4521\n",
      "Epoch 50/50\n",
      "1875/1875 [==============================] - 1s 492us/step - loss: 0.0405 - accuracy: 0.4531 - val_loss: 0.0405 - val_accuracy: 0.4484\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss = \"mse\",\n",
    "              optimizer = \"ADAM\",\n",
    "              metrics = [\"accuracy\"])\n",
    "history = model.fit(x = F_train, y = Q_train, validation_data=(F_val, Q_val),epochs = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "95f045bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "625/625 [==============================] - 0s 261us/step - loss: 0.0407 - accuracy: 0.4502\n"
     ]
    }
   ],
   "source": [
    "#F 학습 후 F accuracy\n",
    "result = model.evaluate(F_test, Q_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "7a22dfbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "625/625 [==============================] - 0s 258us/step - loss: 0.1609 - accuracy: 0.2206\n"
     ]
    }
   ],
   "source": [
    "#F 학습 후 nF accuracy\n",
    "result = model.evaluate(nF_test, Q_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "6e08999e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "1875/1875 [==============================] - 1s 570us/step - loss: 0.0522 - accuracy: 0.3321 - val_loss: 0.0506 - val_accuracy: 0.3341\n",
      "Epoch 2/50\n",
      "1875/1875 [==============================] - 1s 477us/step - loss: 0.0508 - accuracy: 0.3319 - val_loss: 0.0506 - val_accuracy: 0.3332\n",
      "Epoch 3/50\n",
      "1875/1875 [==============================] - 1s 483us/step - loss: 0.0507 - accuracy: 0.3318 - val_loss: 0.0506 - val_accuracy: 0.3286\n",
      "Epoch 4/50\n",
      "1875/1875 [==============================] - 1s 486us/step - loss: 0.0508 - accuracy: 0.3328 - val_loss: 0.0506 - val_accuracy: 0.3284\n",
      "Epoch 5/50\n",
      "1875/1875 [==============================] - 1s 478us/step - loss: 0.0507 - accuracy: 0.3340 - val_loss: 0.0506 - val_accuracy: 0.3338\n",
      "Epoch 6/50\n",
      "1875/1875 [==============================] - 1s 486us/step - loss: 0.0507 - accuracy: 0.3330 - val_loss: 0.0506 - val_accuracy: 0.3323\n",
      "Epoch 7/50\n",
      "1875/1875 [==============================] - 1s 482us/step - loss: 0.0507 - accuracy: 0.3319 - val_loss: 0.0507 - val_accuracy: 0.3344\n",
      "Epoch 8/50\n",
      "1875/1875 [==============================] - 1s 496us/step - loss: 0.0507 - accuracy: 0.3314 - val_loss: 0.0506 - val_accuracy: 0.3356\n",
      "Epoch 9/50\n",
      "1875/1875 [==============================] - 1s 487us/step - loss: 0.0507 - accuracy: 0.3309 - val_loss: 0.0506 - val_accuracy: 0.3329\n",
      "Epoch 10/50\n",
      "1875/1875 [==============================] - 1s 481us/step - loss: 0.0507 - accuracy: 0.3322 - val_loss: 0.0506 - val_accuracy: 0.3297\n",
      "Epoch 11/50\n",
      "1875/1875 [==============================] - 1s 491us/step - loss: 0.0507 - accuracy: 0.3340 - val_loss: 0.0506 - val_accuracy: 0.3281\n",
      "Epoch 12/50\n",
      "1875/1875 [==============================] - 1s 499us/step - loss: 0.0507 - accuracy: 0.3323 - val_loss: 0.0506 - val_accuracy: 0.3360\n",
      "Epoch 13/50\n",
      "1875/1875 [==============================] - 1s 512us/step - loss: 0.0507 - accuracy: 0.3315 - val_loss: 0.0506 - val_accuracy: 0.3352\n",
      "Epoch 14/50\n",
      "1875/1875 [==============================] - 1s 502us/step - loss: 0.0507 - accuracy: 0.3332 - val_loss: 0.0506 - val_accuracy: 0.3349\n",
      "Epoch 15/50\n",
      "1875/1875 [==============================] - 1s 496us/step - loss: 0.0507 - accuracy: 0.3336 - val_loss: 0.0507 - val_accuracy: 0.3329\n",
      "Epoch 16/50\n",
      "1875/1875 [==============================] - 1s 511us/step - loss: 0.0507 - accuracy: 0.3322 - val_loss: 0.0506 - val_accuracy: 0.3338\n",
      "Epoch 17/50\n",
      "1875/1875 [==============================] - 1s 496us/step - loss: 0.0507 - accuracy: 0.3341 - val_loss: 0.0506 - val_accuracy: 0.3322\n",
      "Epoch 18/50\n",
      "1875/1875 [==============================] - 1s 492us/step - loss: 0.0507 - accuracy: 0.3330 - val_loss: 0.0506 - val_accuracy: 0.3347\n",
      "Epoch 19/50\n",
      "1875/1875 [==============================] - 1s 487us/step - loss: 0.0507 - accuracy: 0.3334 - val_loss: 0.0506 - val_accuracy: 0.3334\n",
      "Epoch 20/50\n",
      "1875/1875 [==============================] - 1s 485us/step - loss: 0.0507 - accuracy: 0.3324 - val_loss: 0.0506 - val_accuracy: 0.3317\n",
      "Epoch 21/50\n",
      "1875/1875 [==============================] - 1s 493us/step - loss: 0.0507 - accuracy: 0.3339 - val_loss: 0.0506 - val_accuracy: 0.3349\n",
      "Epoch 22/50\n",
      "1875/1875 [==============================] - 1s 490us/step - loss: 0.0507 - accuracy: 0.3332 - val_loss: 0.0506 - val_accuracy: 0.3356\n",
      "Epoch 23/50\n",
      "1875/1875 [==============================] - 1s 485us/step - loss: 0.0507 - accuracy: 0.3336 - val_loss: 0.0506 - val_accuracy: 0.3337\n",
      "Epoch 24/50\n",
      "1875/1875 [==============================] - 1s 489us/step - loss: 0.0507 - accuracy: 0.3336 - val_loss: 0.0506 - val_accuracy: 0.3338\n",
      "Epoch 25/50\n",
      "1875/1875 [==============================] - 1s 489us/step - loss: 0.0507 - accuracy: 0.3334 - val_loss: 0.0506 - val_accuracy: 0.3377\n",
      "Epoch 26/50\n",
      "1875/1875 [==============================] - 1s 485us/step - loss: 0.0507 - accuracy: 0.3328 - val_loss: 0.0506 - val_accuracy: 0.3302\n",
      "Epoch 27/50\n",
      "1875/1875 [==============================] - 1s 477us/step - loss: 0.0507 - accuracy: 0.3346 - val_loss: 0.0506 - val_accuracy: 0.3308\n",
      "Epoch 28/50\n",
      "1875/1875 [==============================] - 1s 475us/step - loss: 0.0507 - accuracy: 0.3340 - val_loss: 0.0506 - val_accuracy: 0.3309\n",
      "Epoch 29/50\n",
      "1875/1875 [==============================] - 1s 483us/step - loss: 0.0507 - accuracy: 0.3345 - val_loss: 0.0506 - val_accuracy: 0.3343\n",
      "Epoch 30/50\n",
      "1875/1875 [==============================] - 1s 480us/step - loss: 0.0507 - accuracy: 0.3336 - val_loss: 0.0506 - val_accuracy: 0.3343\n",
      "Epoch 31/50\n",
      "1875/1875 [==============================] - 1s 479us/step - loss: 0.0507 - accuracy: 0.3332 - val_loss: 0.0506 - val_accuracy: 0.3331\n",
      "Epoch 32/50\n",
      "1875/1875 [==============================] - 1s 475us/step - loss: 0.0507 - accuracy: 0.3327 - val_loss: 0.0505 - val_accuracy: 0.3320\n",
      "Epoch 33/50\n",
      "1875/1875 [==============================] - 1s 474us/step - loss: 0.0507 - accuracy: 0.3330 - val_loss: 0.0506 - val_accuracy: 0.3313\n",
      "Epoch 34/50\n",
      "1875/1875 [==============================] - 1s 480us/step - loss: 0.0507 - accuracy: 0.3342 - val_loss: 0.0506 - val_accuracy: 0.3335\n",
      "Epoch 35/50\n",
      "1875/1875 [==============================] - 1s 483us/step - loss: 0.0507 - accuracy: 0.3335 - val_loss: 0.0506 - val_accuracy: 0.3320\n",
      "Epoch 36/50\n",
      "1875/1875 [==============================] - 1s 482us/step - loss: 0.0507 - accuracy: 0.3325 - val_loss: 0.0506 - val_accuracy: 0.3356\n",
      "Epoch 37/50\n",
      "1875/1875 [==============================] - 1s 481us/step - loss: 0.0507 - accuracy: 0.3324 - val_loss: 0.0505 - val_accuracy: 0.3327\n",
      "Epoch 38/50\n",
      "1875/1875 [==============================] - 1s 476us/step - loss: 0.0507 - accuracy: 0.3343 - val_loss: 0.0507 - val_accuracy: 0.3341\n",
      "Epoch 39/50\n",
      "1875/1875 [==============================] - 1s 477us/step - loss: 0.0507 - accuracy: 0.3334 - val_loss: 0.0506 - val_accuracy: 0.3313\n",
      "Epoch 40/50\n",
      "1875/1875 [==============================] - 1s 489us/step - loss: 0.0507 - accuracy: 0.3339 - val_loss: 0.0506 - val_accuracy: 0.3356\n",
      "Epoch 41/50\n",
      "1875/1875 [==============================] - 1s 494us/step - loss: 0.0507 - accuracy: 0.3330 - val_loss: 0.0506 - val_accuracy: 0.3314\n",
      "Epoch 42/50\n",
      "1875/1875 [==============================] - 1s 482us/step - loss: 0.0507 - accuracy: 0.3333 - val_loss: 0.0506 - val_accuracy: 0.3357\n",
      "Epoch 43/50\n",
      "1875/1875 [==============================] - 1s 484us/step - loss: 0.0507 - accuracy: 0.3345 - val_loss: 0.0506 - val_accuracy: 0.3338\n",
      "Epoch 44/50\n",
      "1875/1875 [==============================] - 1s 476us/step - loss: 0.0507 - accuracy: 0.3339 - val_loss: 0.0506 - val_accuracy: 0.3332\n",
      "Epoch 45/50\n",
      "1875/1875 [==============================] - 1s 480us/step - loss: 0.0507 - accuracy: 0.3338 - val_loss: 0.0505 - val_accuracy: 0.3316\n",
      "Epoch 46/50\n",
      "1875/1875 [==============================] - 1s 489us/step - loss: 0.0507 - accuracy: 0.3337 - val_loss: 0.0505 - val_accuracy: 0.3342\n",
      "Epoch 47/50\n",
      "1875/1875 [==============================] - 1s 496us/step - loss: 0.0507 - accuracy: 0.3337 - val_loss: 0.0506 - val_accuracy: 0.3365\n",
      "Epoch 48/50\n",
      "1875/1875 [==============================] - 1s 484us/step - loss: 0.0507 - accuracy: 0.3337 - val_loss: 0.0506 - val_accuracy: 0.3314\n",
      "Epoch 49/50\n",
      "1875/1875 [==============================] - 1s 482us/step - loss: 0.0507 - accuracy: 0.3348 - val_loss: 0.0505 - val_accuracy: 0.3340\n",
      "Epoch 50/50\n",
      "1875/1875 [==============================] - 1s 483us/step - loss: 0.0507 - accuracy: 0.3343 - val_loss: 0.0506 - val_accuracy: 0.3352\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss = \"mse\",\n",
    "              optimizer = \"adam\",\n",
    "              metrics = [\"accuracy\"])\n",
    "history = model.fit(x = nF_train, y = Q_train, validation_data=(nF_val, Q_val),epochs = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "6e80a0d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "625/625 [==============================] - 0s 264us/step - loss: 0.0508 - accuracy: 0.3363\n"
     ]
    }
   ],
   "source": [
    "#nF 학습 후 nF accuracy\n",
    "result = model.evaluate(nF_test, Q_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd712eab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
